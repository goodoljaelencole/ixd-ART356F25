 
### Chapter 5, Pages 162–191

Reading this section completely changed how I think about “human error.” Norman makes a clear point that stuck with me: when 75–95% of industrial accidents are blamed on people, the problem usually isn’t people—it’s design. I’ve always grown up hearing “be careful” or “don’t mess up,” as if errors come from carelessness. But Norman flips that idea. He shows that most systems are basically traps waiting for the next tired, distracted, or overloaded human to fall into them.

The examples he gives—like forgetting whether a stove burner is on, turning off the wrong device, or misreading a control—felt familiar. They aren’t dramatic failures; they’re normal human behavior. That’s what hit me. Designers often expect a level of perfect attention that no one can actually deliver. We get interrupted, we get tired, we multitask, and we rely on habits. Yet when accidents happen, companies blame people instead of the environment or the design that pushed them into those mistakes.

His distinction between **slips** and **mistakes** also helped me understand my own errors better. Slips happen when the intention is correct but the action goes wrong—like putting milk in the pantry instead of the fridge. Mistakes happen when we choose the wrong plan from the start. I realized I often blame myself harshly even for slips, which are basically the brain running on autopilot. Norman makes it clear these should be expected and planned for, not punished.

Another part that stood out was the section on social and institutional pressures. The stories of aviation incidents, medical errors, and workplace violations showed that the “human error” label is often just a shortcut to avoid deeper systemic change. People cut corners because the system forces them to. They face contradictory demands: follow all the rules, but also be fast, efficient, and perfect. It’s easy to see how someone messes up, and even easier to see why blaming them doesn’t fix anything.

The strongest takeaway for me is that **good design assumes failure**. Not in a pessimistic way, but in a realistic way. If a system requires constant attention, perfect memory, and flawless judgment, it’s already broken. Norman makes a compelling case that true human-centered design works *with* human tendencies, not against them. Reading this chapter made me more aware of how many of my own frustrations with devices, apps, and even school systems come from designs that ignore how people actually think and behave.

Overall, Chapter 5 pushed me to see error differently—not as a personal flaw, but as a signal that something upstream needs fixing. It’s a mindset shift that feels both empowering and a little relieving.
